{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is a simple implementation for scrapping multiple web pages. I built this implementation to collect some data which I was trying to get for building a deep learning model.\n",
    "### In this implementation URL's to be scrapped are picked from a JSON file, each URL's web content is then scrapped and stored in a seperate file.\n",
    "### So, I would browse through different web pages from which I wanted to scrap content, then through a chrome plugin I would extract my browsing history as JSON file and feel to this program.\n",
    "\n",
    "### Rough but works !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting scraping of 1 pages\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "urls_to_be_scrapped = []\n",
    "with open('./input/history.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for records in data:\n",
    "        urls_to_be_scrapped.append(records['url'])\n",
    "\n",
    "print ('starting scraping of %d pages'%(len(urls_to_be_scrapped)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the library used to query a website\n",
    "import urllib.request #if you are using python3+ version, import urllib.request\n",
    "\n",
    "#import the Beautiful soup functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "\n",
    "def scrap_page(pages,counter):\n",
    "    \n",
    "    for page in pages:\n",
    "        counter = np.random.randint(0,32000)\n",
    "        o = urlparse(page)\n",
    "        domain = o.netloc\n",
    "        fn = './outputs/scrapped_text_'+domain+'_'+str(counter)+'.txt'\n",
    "        scrapped_txt_file = open(fn,'w',encoding='utf-8')\n",
    "        time.sleep(5)\n",
    "        print ('scrapping ', page)\n",
    "        \n",
    "        try:\n",
    "            #Query the website and return the html to the variable 'page'\n",
    "            page = urllib.request.urlopen(page) #For python 3 use urllib.request.urlopen(wiki)\n",
    "        except urllib.request.HTTPError:\n",
    "            print ('got http error skipping this page')\n",
    "            continue\n",
    "        #Parse the html in the 'page' variable, and store it in Beautiful Soup format\n",
    "        soup = BeautifulSoup(page)\n",
    "\n",
    "        paras = soup.find_all('p')\n",
    "\n",
    "        for para in paras:\n",
    "            item = para.find(text=True)\n",
    "            if (item is not None):\n",
    "                scrapped_txt_file.write(item)\n",
    "                scrapped_txt_file.write('\\n')\n",
    "            \n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrapping  https://en.wikipedia.org/wiki/Hedge_fund\n"
     ]
    }
   ],
   "source": [
    "scrap_page(urls_to_be_scrapped,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
